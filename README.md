# ğŸ§ª MFCS-Bench

**MFCS-Bench** is a benchmark system for evaluating large language models (LLMs) on function calling tasks, based on the [MFCS (Model Function Calling Standard)](https://github.com/mfcsorg/mfcs) protocol. It standardizes evaluation of how well different models handle structured function calls, helping build a more robust tool-using LLM ecosystem.

[ä¸­æ–‡æ–‡æ¡£](README_CN.md)

---

## ğŸš€ Features

- âœ… **MFCS Protocol Compatible**: Unified interface for evaluating function calls across different LLMs
- ğŸ“Š **Comprehensive Metrics**: Tool usage rate, semantic match rate, accuracy, and response time
- ğŸ”„ **Streaming Support**: Real-time response analysis with streaming output
- ğŸ“ˆ **Detailed Reports**: Both summary and detailed markdown reports with test analytics
- ğŸ” **Automated Pipeline**: Fully automated benchmark workflow

---

## ğŸ“¦ Installation

```bash
git clone https://github.com/your-org/mfcs-bench.git
cd mfcs-bench
pip install -e .
```

---

## ğŸ”§ Quick Start

1. Configure your test cases in `test_cases/` directory
2. Set up your application config in `apps/config.json`
3. Run the benchmark:

```bash
pip install -r apps/mfcs-python/requirements.txt
python run_benchmark.py
```

Results will be saved to the `reports/` directory with timestamp-based filenames:
- `summary_YYYYMMDD_HHMMSS.md`: Overall benchmark results
- `report_YYYYMMDD_HHMMSS.md`: Detailed test analysis

---

## ğŸ“ Project Structure

```
mfcs-bench/
â”œâ”€â”€ apps/              # Application configurations and examples
â”‚   â”œâ”€â”€ config.json    # Main configuration file
â”‚   â”œâ”€â”€ mfcs-python/   # Python implementation examples
â”‚   â””â”€â”€ mfcs-js/       # JavaScript implementation examples
â”œâ”€â”€ reports/           # Generated benchmark reports
â”œâ”€â”€ src/              
â”‚   â””â”€â”€ mfcs_bench/    # Core benchmark implementation
â”‚       â””â”€â”€ core/      # Analysis and processing logic
â”œâ”€â”€ test_cases/        # Test case definitions
â””â”€â”€ run_benchmark.py   # Main entry point
```

---

## ğŸ“Š Evaluation Metrics

| Metric                | Description                                          |
|----------------------|------------------------------------------------------|
| Tool Usage Rate      | Percentage of correct tool usage in responses        |
| Semantic Match Rate  | Accuracy of semantic content matching                |
| Response Time        | Average time taken to generate responses             |
| Token Usage          | Prompt and completion token consumption              |
| Success Rate         | Overall test case success percentage                 |

---

## ğŸ” Test Case Format

Test cases are defined in JSON format:

```json
{
    "input": {
        "user": "example query"
    },
    "expected_output": {
        "semantic_match": "expected response",
        "contains_tool": "get_weather"
    },
    "description": "Test case description"
}
```

---

## ğŸ“¢ Contribute

We welcome contributions!

- Add new test cases
- Improve evaluation metrics
- Enhance report generation
- Add support for more LLM implementations

---

## ğŸ“œ License

MIT License
